{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Lets import all the libraties needed\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pegassos and Mercer_Pegasos\n",
    "Classes Pegassos and Mercer_Pegasos are declared in the file pegasos.py\n",
    "\n",
    "So instead of declaring these classes in the following cells you can just uncomment the below line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pegasos import Pegasos, Mercer_Pegasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets see a simple Pegasos class and its functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pegasos(object):\n",
    "    def __init__(self, n_iter=10, lambda1=1, projection=False, bias = False, objective='hinge', margin = 0.2):\n",
    "#         Initialise the variables needed\n",
    "        self.n_iter = n_iter # number of iterations\n",
    "        self._lambda = lambda1 # parameter that controls the descent \n",
    "        self.projection = projection # Optional projection step\n",
    "        self.labelEncoder = None # encodesthe labels to 0, 1\n",
    "        self.bias = bias # Set it to true if a bias variable has to be added.\n",
    "        self.objective = objective # Objective function. can take values ['hinge', 'sigmoid', 'margin']\n",
    "        self.margin = margin # size of the margin needed if the objective function is set to 'margin' (epislon value)\n",
    " \n",
    "    def fit(self, X, Y, verbose=False):\n",
    "        starttime = time.time()\n",
    "        \n",
    "#         Encode the labels\n",
    "        self.labelEncoder = preprocessing.LabelEncoder()\n",
    "        self.labelEncoder.fit(Y)\n",
    "        Y_labels = 2*self.labelEncoder.transform(Y) - 1\n",
    "        \n",
    "#         Add 1 at the end if bias is needed\n",
    "        if (self.bias):\n",
    "            X = np.append(X, np.ones(X.shape[0]).reshape(-1, 1), axis = 1)\n",
    "            \n",
    "#         initialize w\n",
    "        self.w = np.zeros((X.shape[1]))\n",
    "        \n",
    "        for t in range(self.n_iter):\n",
    "            i_t =  random.randint(0, X.shape[0] - 1)\n",
    "            x = X[i_t]\n",
    "            y = Y_labels[i_t]\n",
    "                            \n",
    "            eta = 1.0/float(self._lambda * (t+1))\n",
    "            \n",
    "#             Depending on the different loss functions, we update self.w with respective sub-gradients\n",
    "            if(self.objective == 'hinge'):\n",
    "                if((y*( self.w.dot(x)))  < 1):\n",
    "                    self.w = (1 - eta*self._lambda)*self.w + eta*y*((x))\n",
    "                else:\n",
    "                    self.w = (1 - eta*self._lambda)*self.w\n",
    "            \n",
    "            elif (self.objective == 'sigmoid'):\n",
    "                self.w = (1 - eta*self._lambda)*self.w + eta* (y/(1 + np.exp((y*( self.w.dot(x)))))) * x\n",
    "            \n",
    "                \n",
    "            elif (self.objective == 'margin'):\n",
    "                if(self.w.dot(x) - y > self.margin):\n",
    "                    self.w = (1 - eta*self._lambda)*self.w + ((x))\n",
    "                elif(y - self.w.dot(x) > self.margin):\n",
    "                    self.w = (1 - eta*self._lambda)*self.w - ((x))\n",
    "                    \n",
    "            if(self.projection):\n",
    "                self.w = (np.min((((1.0/ np.sqrt(self._lambda))/np.linalg.norm(self.w)), 1.0))) * self.w\n",
    "                \n",
    "        if(verbose):\n",
    "            print(\"Total time take to train data of size \" + str(X.shape[0]) + \" datapoints with \" +\n",
    "             str(X.shape[1]) + \" features is \" + str(time.time() - starttime) + \" seconds\")\n",
    "        \n",
    "            \n",
    "    def predict(self, X):\n",
    "        if(self.bias) :\n",
    "            X = np.append(X, np.ones(X.shape[0]).reshape(-1, 1), axis = 1)\n",
    "        Ypred = (((X)@self.w))\n",
    "        Y =  clf.labelEncoder.inverse_transform(((1 + (np.sign(Ypred)))/2).astype(int))\n",
    "        return Y\n",
    "    \n",
    "    def test(self, X, Y, verbose=False):\n",
    "        starttime = time.time()\n",
    "        \n",
    "        if(self.bias) :\n",
    "            X = np.append(X, np.ones(X.shape[0]).reshape(-1, 1), axis = 1)\n",
    "        Ypred = (((X)@self.w))\n",
    "        Y =  clf.labelEncoder.inverse_transform(((1 + (np.sign(Ypred)))/2).astype(int))\n",
    "        \n",
    "        accuracy = accuracy_score(Y, testY)\n",
    "        f1 = f1_score(Y, testY)\n",
    "        \n",
    "        if(verbose):\n",
    "            print(\"Total time take to test on  data of size \" + str(X.shape[0]) + \" datapoints with \" +\n",
    "             str(X.shape[1]) + \" features is \" + str(time.time() - starttime) + \" seconds\")\n",
    "        \n",
    "        \n",
    "        print(\"Accuracy is : \" + str(accuracy))\n",
    "        print(\"F1 score is : \" + str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see the entire procedure.\n",
    "\n",
    "## Subgradient Update\n",
    "\n",
    "In the function fit() in the above cell, self.w is updated in the below form\n",
    "~~~~\n",
    "eta = 1.0/float(self._lambda * (t+1))\n",
    "self.w = (1 - eta*self._lambda)*self.w + eta*(sub_gradient)\n",
    "~~~~\n",
    "This process is strickingly similar to SGD. But here the step size is carefully chosen so that we can have an upperbound on the number of iterations needed. We can see that as the number of timesteps t increases, eta decreases and we keep taking smaller steps towards the goal. It makes sense because intially we are far off the goal, so we have to take higher steps, but more we go towards the goal, we need to keep taking fewer steps. \n",
    "\n",
    "## Projection Step\n",
    "\n",
    "`self.w = (np.min((((1.0/ np.sqrt(self._lambda))/np.linalg.norm(self.w)), 1.0))) * self.w`\n",
    "\n",
    "In cases of outliers, they give us a gradient direction, thats very far off. So in order to handle cases like this, we restrict set of admissable values to a circle if radius (1/sqrt(lambda))\n",
    "\n",
    "## Objective functions \n",
    "\n",
    "Objective functions or loss functions tell you how far off you are from the goal. Our objective is the minimize this fucntion globally. But in algorithms like this, we can only achieve local minima. \n",
    "Given below are some objective functions and their subgradients. \n",
    "\n",
    "<img src=\"images/lossfunctions.png\">\n",
    "\n",
    "In this code, the first three are implemented. as 'hinge', 'sigmoid', 'margin' losses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Lets run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()\n",
    "trainX = trainX.reshape(trainX.shape[0], -1)\n",
    "testX = testX.reshape(testX.shape[0], -1)\n",
    "\n",
    "trainX = np.copy(trainX)\n",
    "trainY = np.copy(trainY)\n",
    "testX = np.copy(testX)\n",
    "testY = np.copy(testY)\n",
    "\n",
    "trainY[trainY > 0] = 1\n",
    "testY[testY > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time take to train data of size 60000 datapoints with 784 features is 0.003719806671142578 seconds\n"
     ]
    }
   ],
   "source": [
    "# Call a linear  Pagasos object \n",
    "clf = Pegasos( n_iter=100, objective='hinge')\n",
    "\n",
    "# Fit the algortihm.\n",
    "# Set verbose=True if you want to display fitting time and dataset information.\n",
    "clf.fit(trainX, trainY, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjan/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Predict on a given testdata\n",
    "Ypred = clf.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time take to test on  data of size 10000 datapoints with 784 features is 0.08300113677978516 seconds\n",
      "Accuracy is : 0.9419\n",
      "F1 score is : 0.9675690761931341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjan/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#Run metrics on the given testdata\n",
    "# Set verbose=True if you want to display inference time and dataset information.\n",
    "clf.test(testX, testY, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Now lets see the results with  different objective fucntions\n",
    "<img src=\"images/table1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets see a Kernalized Pegasos class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mercer_Pegasos(Pegasos):\n",
    "    def __init__(self, n_iter=10, lambda1=1, projection=False, bias = False, kernel = None):\n",
    "        super().__init__(n_iter, lambda1, projection, bias) # Variables same as in linear Pegasos\n",
    "        if not (kernel):    # if not kernal is specified, guassian rbf kernalis used by default\n",
    "            kernel = self.rbf\n",
    "        self.kernel = kernel\n",
    "        \n",
    "    def fit(self, X, Y, verbose=False):\n",
    "        starttime = time.time()\n",
    "        \n",
    "        self.labelEncoder = preprocessing.LabelEncoder()\n",
    "        self.labelEncoder.fit(Y)\n",
    "        Y_labels = 2*self.labelEncoder.transform(Y) - 1\n",
    "        self.w = np.zeros((X.shape[1]))\n",
    "        self.alpha = np.zeros((X.shape[0]))\n",
    "        self.X = X\n",
    "        \n",
    "        for t in range(self.n_iter):\n",
    "            i_t =  random.randint(0, X.shape[0] - 1)\n",
    "            x = X[i_t]\n",
    "            y = Y_labels[i_t]\n",
    "            \n",
    "            eta = 1.0/float(self._lambda * (t+1))\n",
    "            \n",
    "            error = 0\n",
    "            for j in range(X.shape[0]):\n",
    "                error += self.alpha[j] * y * (self.kernel(x, X[j]))\n",
    "            if( (y*(1/self._lambda)* error) < 1):\n",
    "                self.alpha[i_t]+=1\n",
    "        if(verbose):\n",
    "            print(\"Total time take to train data of size \" + str(X.shape[0]) + \" datapoints with \" +\n",
    "             str(X.shape[1]) + \" features over \" +str(self.n_iter) +  \" iterations is \" + str(time.time() - starttime) + \" seconds\")\n",
    "            \n",
    "    def predict(self, X, ):\n",
    "        Ypred = np.zeros((X.shape[0]))\n",
    "        for i in range(X.shape[0]) :\n",
    "            wTx = 0\n",
    "            for j in range(self.X.shape[0]) :\n",
    "                wTx += self.alpha[j] * self.kernel(X[i], self.X[j])\n",
    "            Ypred[i] = np.sign(wTx)\n",
    "        Ypred[Ypred > 0 ] = 1\n",
    "        Ypred[Ypred <= 0 ] = 0\n",
    "        return Ypred\n",
    "    \n",
    "    def test(self, X, Y, verbose = False):\n",
    "        starttime = time.time()\n",
    "        \n",
    "        Ypred = np.zeros((X.shape[0]))\n",
    "        for i in range(X.shape[0]) :\n",
    "            wTx = 0\n",
    "            for j in range(self.X.shape[0]) :\n",
    "                wTx += self.alpha[j] * self.kernel(X[i], self.X[j])\n",
    "            Ypred[i] = np.sign(wTx)\n",
    "        Ypred[Ypred > 0 ] = 1\n",
    "        Ypred[Ypred <= 0 ] = 0\n",
    "\n",
    "        Y =  clf.labelEncoder.inverse_transform(Ypred.astype(int))\n",
    "        \n",
    "        accuracy = accuracy_score(Y, testY)\n",
    "        f1 = f1_score(Y, testY)\n",
    "        \n",
    "        print(\"Accuracy is : \" + str(accuracy))\n",
    "        print(\"F1 score is : \" + str(f1))\n",
    "            \n",
    "        if(verbose):\n",
    "            print(\"Total time take to test on  data of size \" + str(X.shape[0]) + \" datapoints with \" +\n",
    "             str(X.shape[1]) + \" features is \" + str(time.time() - starttime) + \" seconds\")\n",
    "            \n",
    "    def rbf(self, x1, x2, gamma = 0.5):\n",
    "        return np.exp(-gamma* np.linalg.norm(x1-x2) * np.linalg.norm(x1-x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with kernels\n",
    "Major difference in kernalized version of any algorithm is that we dont explicitly have the function mapping. So we cannot store w and multiply it with out feature vector phi(x). becuase we do not have the function phi(). \n",
    "\n",
    "But we have a function K() such that K(x1,x2) = phi(x1) * phi(x2)\n",
    "\n",
    "## Solution\n",
    "In the linear algorithm w is updates as below. \n",
    "\n",
    "~~~~\n",
    "self.w = (1 - eta*self._lambda)*self.w + eta*y*((x))\n",
    "~~~~\n",
    "\n",
    "So we take advantage of the fact that in the linear version of the algorithm, w (ie self.w) starting from 0, is always added our featurevector multiplied with some number. Hence w is a linear combination of our feature vectors. So it is sufficient to store the weights (alpha) for each feature vector in the training set, so that we can reconstruct w later by multiplying corresponding alpha and featurevector x. \n",
    "\n",
    "( ( alpha(1) \\* X(1) ) + ( alpha(2) \\* X(2)) ......) \\* X_test   \n",
    "\n",
    "                        =  ( alpha(1) * (X(1) * Xtest) ) + ( alpha(2) * (X(2) * Xtest) ) ......) \n",
    "\n",
    "                        =  ( alpha(1) * (K(X(1),Xtest)) ) + ( alpha(2) * (K(X(2),Xtest))  ) ......) \n",
    "\n",
    "Hence it is sufficient to store the alpha array. But this has an obvious disadvantage of too much inference time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading kernels \n",
    "\n",
    "The kernals are implemented in the file kernels.py \n",
    "\n",
    "So instead of declaring these fucntions in the following cells you can just uncomment the below line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kernels import rbf, polynomialkernel, laplacerbfkernel, sigmoidkernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf(x1, x2, gamma = 0.5):\n",
    "    return np.exp(-gamma* np.linalg.norm(x1-x2) * np.linalg.norm(x1-x2))\n",
    "\n",
    "def polynomialkernel(x1, x2, d =3):\n",
    "    return np.power(x1.dot(x2) + 1, d)\n",
    "\n",
    "def laplacerbfkernel(x1, x2, sigma = 0.1):\n",
    "    return np.exp((-1/sigma) * np.linalg.norm(x1-x2))\n",
    "\n",
    "def sigmoidkernel(x1, x2, alpha=0.1, c=0):\n",
    "    return np.tanh(alpha * x1.dot(x2) + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data. We will be loading a coparitively smaller dataset because kernalized Pegassos is slow\n",
    "dataset = load_breast_cancer()\n",
    "X = dataset.data\n",
    "Y = dataset.target\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time take to train data of size 455 datapoints with 30 features over 100 iterations is 0.6901099681854248 seconds\n"
     ]
    }
   ],
   "source": [
    "# Call a linear  Pagasos object \n",
    "clf = Mercer_Pegasos( n_iter=100, kernel=rbf)\n",
    "\n",
    "# Fit the algortihm.\n",
    "# Set verbose=True if you want to display fitting time and dataset information.\n",
    "clf.fit(trainX, trainY, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on a given testdata\n",
    "Ypred = clf.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 0.8859649122807017\n",
      "F1 score is : 0.906474820143885\n",
      "Total time take to test on  data of size 114 datapoints with 30 features is 0.7540173530578613 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjan/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#Run metrics on the given testdata\n",
    "# Set verbose=True if you want to display inference time and dataset information.\n",
    "clf.test(testX, testY, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Now lets see the results with  different objective kernels\n",
    "<img src=\"images/table2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
